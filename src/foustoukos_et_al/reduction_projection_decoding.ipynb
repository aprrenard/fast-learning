{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection on learning dimension and decoding\n",
    "\n",
    "Implement PCA or other dimensionality reduction techniques.\n",
    "Project activity on \"learning\" dimension defined as difference between meaningful average population response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import src.utils.utils_io as io\n",
    "import src.utils.utils_imaging as imaging_utils\n",
    "\n",
    "\n",
    "# Path to the directory containing the processed data.\n",
    "processed_dir = r\"//sv-nas1.rcp.epfl.ch/Petersen-Lab/analysis/Anthony_Renard/data_processed/mice\"\n",
    "\n",
    "# Session metadata file.\n",
    "db_path = r\"//sv-nas1.rcp.epfl.ch/Petersen-Lab/analysis/Anthony_Renard/mice_info/session_metadata.xlsx\"\n",
    "\n",
    "# Rewarded and non-rewarded NWB files.\n",
    "group_yaml_rew = r\"//sv-nas1.rcp.epfl.ch/Petersen-Lab/analysis/Anthony_Renard/mice_info/groups/imaging_rewarded.yaml\"\n",
    "group_yaml_non_rew = r\"//sv-nas1.rcp.epfl.ch/Petersen-Lab/analysis/Anthony_Renard/mice_info/groups/imaging_non_rewarded.yaml\"\n",
    "nwb_list_rew = io.read_group_yaml(group_yaml_rew)\n",
    "nwb_list_non_rew = io.read_group_yaml(group_yaml_non_rew)\n",
    "nwb_list = nwb_list_rew + nwb_list_non_rew\n",
    "\n",
    "\n",
    "def reduce_dimensionality(activity, win, n_days, n_trials, fitting_type='single_bin'):\n",
    "    \n",
    "    if fitting_type == 'single_bin':\n",
    "        X = np.mean(activity[:, :, :, win[0]:win[1]], axis=3)\n",
    "        X = np.reshape(X, (X.shape[0], -1))\n",
    "    elif fitting_type == 'psth':\n",
    "        X = np.mean(activity, axis=2)\n",
    "    else:\n",
    "        raise ValueError('Unknown fitting type.')\n",
    "    \n",
    "    # Transpose to (n_samples, n_features).\n",
    "    X = X.T\n",
    "    # z-score the data.\n",
    "    X = StandardScaler(with_mean=True, with_std=True).fit_transform(X)\n",
    "    # Perform PCA.\n",
    "    pca = PCA(n_components=None)\n",
    "    model = pca.fit(X)\n",
    "    \n",
    "    # Apply the model.\n",
    "    # Reshape activity to (n_neurons, n_trials x n_timepoints) and transform.\n",
    "    reduced_act = model.transform(activity.reshape(activity.shape[0], -1).T)\n",
    "    reduced_act = reduced_act.T\n",
    "    n_pc = model.n_components_\n",
    "    s = activity.shape\n",
    "    # First dim length is min(n_features, n_samples).\n",
    "    # Add session dimension.\n",
    "    reduced_act = reduced_act.reshape(n_pc, n_days, n_trials, s[2])\n",
    "    \n",
    "    return reduced_act, model\n",
    "\n",
    "\n",
    "def pc_to_retain(model, threshold):\n",
    "    \n",
    "    mask = model.explained_variance_ratio_.cumsum() < threshold\n",
    "    # Also select the first PC that crosses the threshold.\n",
    "    pc_that_cross = (mask == False).argmax()\n",
    "    mask[pc_that_cross] = True\n",
    "    \n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. PCA and PC PSTH's\n",
    "\n",
    "PCA in performed on the mapping trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aprenard\\Anaconda3\\envs\\fast-learning\\Lib\\site-packages\\openpyxl\\worksheet\\_read_only.py:81: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 30\n",
    "trial_type = 'UM'\n",
    "n_trials = 50\n",
    "n_days = 5\n",
    "win = (1, 2)  # from stimulus onset to 300 ms after.\n",
    "win = (int(win[0] * sampling_rate), int(win[1] * sampling_rate))\n",
    "baseline = (0, 1)\n",
    "baseline = (int(baseline[0] * sampling_rate), int(baseline[1] * sampling_rate))\n",
    "\n",
    "mouse_list = ['AR127']\n",
    "\n",
    "session_list = io.select_sessions_from_db(db_path,\n",
    "                                          experimenters=['AR', 'GF', 'MI'],\n",
    "                                          exclude_cols=['exclude', 'two_p_exclude'],\n",
    "                                          subject_id=mouse_list,\n",
    "                                          reward_group='R+',\n",
    "                                          day=['-2', '-1', '0', '+1', '+2'],\n",
    "                                          two_p_imaging='yes')\n",
    "\n",
    "activity = imaging_utils.shape_features_matrix(mouse_list, session_list, processed_dir, trial_type, 50)\n",
    "# Subtract baselines.\n",
    "activity = activity - np.nanmean(activity[:, :, baseline[0]:baseline[1]],\n",
    "                                 axis=2, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape feature matrix.\n",
    "Fit PCA by either:\n",
    "- for each neuron, keep all trial and compute the mean response over time.\n",
    "- for each neuron, average trials and keep the mean response over time\n",
    "(keep time dimension).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X = np.mean(activity[:, :, :, win[0]:win[1]], axis=3)\n",
    "X = np.mean(activity, axis=2)\n",
    "X = np.reshape(X, (X.shape[0], -1))\n",
    "\n",
    "# Transpose to (n_samples, n_features).\n",
    "X = X.T\n",
    "# z-score the data.\n",
    "X = StandardScaler(with_mean=True, with_std=True).fit_transform(X)\n",
    "# Perform PCA.\n",
    "pca = PCA(n_components=None)\n",
    "model = pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PC PSTH's\n",
    "Look at the PC's average responses across days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model.\n",
    "# Reshape activity to (n_neurons, n_trials x n_timepoints) and transform.\n",
    "reduced_act = model.transform(activity.reshape(activity.shape[0], -1).T)\n",
    "reduced_act = reduced_act.T\n",
    "n_pc = model.n_components_\n",
    "s = activity.shape\n",
    "# First dim length is min(n_features, n_samples).\n",
    "# Add session dimension.\n",
    "reduced_act = reduced_act.reshape(n_pc, n_days, n_trials, s[2])\n",
    "\n",
    "# Create PC PSTH's. \n",
    "pc_psth = reduced_act - np.mean(reduced_act[:, :, :, baseline[0]:baseline[1]],\n",
    "                                axis=3, keepdims=True)\n",
    "pc_psth = np.mean(reduced_act, axis=2)\n",
    "\n",
    "# Save PC PSTH to pdf.\n",
    "pdf_path = r\"\\\\sv-nas1.rcp.epfl.ch\\Petersen-Lab\\analysis\\Anthony_Renard\\analysis_output\\pca\\pca_exploration.pdf\"\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    for pc in range(30):\n",
    "        f, axes = plt.subplots(1, 5, sharey=True, figsize=(15, 5))\n",
    "        for i in range(n_days):\n",
    "            axes[i].plot(pc_psth[pc, i, :])\n",
    "            axes[i].axvline(30, color='orange')\n",
    "        f.suptitle(f\"PC {pc+1}\")\n",
    "        pdf.savefig(f)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at loadings and explained variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(model.explained_variance_ratio_.cumsum())\n",
    "\n",
    "# Plot loading of first 10 PCs.\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.plot(model.components_[i])\n",
    "    plt.title(f\"PC {i+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2. Similarity of population vectors across learning and projection on learning dimension.\n",
    "\n",
    "I can do projection the difference vector or I can measure similarity.\n",
    "Do we see a discret transition from before to after learning?\n",
    "\n",
    "**How to assess whether there is actually a significant difference between the two vectors to start with?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2503804123.py, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[30], line 37\u001b[1;36m\u001b[0m\n\u001b[1;33m    TODO: # Compute similarity between single trial population response and response after learning.\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Load data needed to compute before and after learning.\n",
    "\n",
    "sampling_rate = 30\n",
    "win = (1, 2)  # from stimulus onset to 300 ms after.\n",
    "win = (int(win[0] * sampling_rate), int(win[1] * sampling_rate))\n",
    "baseline = (0, 1)\n",
    "baseline = (int(baseline[0] * sampling_rate), int(baseline[1] * sampling_rate))\n",
    "n_trials = 50\n",
    "days = ['-2', '-1', '0', '+1', '+2']\n",
    "apply_pca = True\n",
    "variance_to_retain = 0.8\n",
    "\n",
    "mouse_list = ['AR127']\n",
    "\n",
    "session_list = io.select_sessions_from_db(db_path,\n",
    "                                          experimenters=['AR', 'GF', 'MI'],\n",
    "                                          exclude_cols=['exclude', 'two_p_exclude'],\n",
    "                                          subject_id=mouse_list,\n",
    "                                          reward_group='R+',\n",
    "                                          day=['-2', '-1', '0', '+1', '+2'],\n",
    "                                          two_p_imaging='yes')\n",
    "\n",
    "act_map = imaging_utils.shape_features_matrix(mouse_list, session_list, processed_dir, 'UM', n_trials)\n",
    "# Subtract baselines.\n",
    "act_map = act_map - np.nanmean(act_map[:, :, baseline[0]:baseline[1]],\n",
    "                               axis=2, keepdims=True)\n",
    "act_learning = imaging_utils.shape_features_matrix(mouse_list, session_list, processed_dir, 'WH', n_trials)\n",
    "# Subtract baselines.\n",
    "act_learning = act_learning - np.nanmean(act_learning[:, :, baseline[0]:baseline[1]],\n",
    "                               axis=2, keepdims=True)\n",
    "\n",
    "act_map = act_map.reshape(act_map.shape[0], 5, n_trials, -1)\n",
    "act_learning = act_learning.reshape(act_learning.shape[0], 3, n_trials, -1)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: dim reduction must be applied to all data Um and WH at the same time.\n",
    "    \n",
    "# Reduce dimensionality of the data.\n",
    "if apply_pca:\n",
    "    reduced_act_map, model_map = reduce_dimensionality(act_map, win, fitting_type='single_bin')\n",
    "    reduced_act_learning, model_learning = reduce_dimensionality(act_learning, win, fitting_type='single_bin')\n",
    "    reduced_act_map = reduced_act_map[pc_to_retain(model_map, variance_to_retain)]\n",
    "    reduced_act_learning = reduced_act_learning[pc_to_retain(model_map, variance_to_retain)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute similarity between single trial population response and response after learning.\n",
    "vect_after_learning = np.mean(act_map[:, 4, :, win[0]:win[1]], axis=(1,2))\n",
    "print(vect_after_learning.shape)\n",
    "pop_vect_mapping = np.mean(act_map[:, :, :, win[0]:win[1]], axis=(3))\n",
    "pop_vect_learning = np.mean(act_learning[:, :, :, win[0]:win[1]], axis=(3))\n",
    "print(pop_vect_mapping.shape)\n",
    "print(pop_vect_learning.shape)\n",
    "\n",
    "cosine_sim_mapping = np.zeros((pop_vect_mapping.shape[1], n_trials))\n",
    "cosine_sim_learning = np.zeros((pop_vect_learning.shape[1], n_trials))\n",
    "dot_sim_mapping = np.zeros((pop_vect_mapping.shape[1], n_trials))\n",
    "dot_sim_learning = np.zeros((pop_vect_learning.shape[1], n_trials))\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def dot_similarity(a, b):\n",
    "    return np.dot(a, b)\n",
    "\n",
    "for i in range(pop_vect_mapping.shape[1]):\n",
    "    for k in range(n_trials):\n",
    "        cosine_sim_mapping[i, k] = cosine_similarity(pop_vect_mapping[:, i, k],\n",
    "                                                     vect_after_learning)\n",
    "        dot_sim_mapping[i, k] = dot_similarity(pop_vect_mapping[:, i, k],\n",
    "                                               vect_after_learning)\n",
    "\n",
    "for i in range(pop_vect_learning.shape[1]):\n",
    "    for k in range(n_trials):\n",
    "        cosine_sim_learning[i, k] = cosine_similarity(pop_vect_learning[:, i, k],\n",
    "                                                      vect_after_learning)\n",
    "        dot_sim_learning[i, k] = dot_similarity(pop_vect_learning[:, i, k],\n",
    "                                                vect_after_learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity.\n",
    "# Mapping trials of the two pretraining days and then learning interleaved\n",
    "# with mapping trials.\n",
    "cosine_sim = np.concatenate((cosine_sim_mapping[0],\n",
    "                             cosine_sim_mapping[1],\n",
    "                             cosine_sim_learning[0],\n",
    "                             cosine_sim_mapping[2],\n",
    "                             cosine_sim_learning[1],\n",
    "                             cosine_sim_mapping[3],\n",
    "                             cosine_sim_learning[2],\n",
    "                             cosine_sim_mapping[4]), axis=0)\n",
    "dot_sim = np.concatenate((dot_sim_mapping[0],\n",
    "                             dot_sim_mapping[1],\n",
    "                             dot_sim_learning[0],\n",
    "                             dot_sim_mapping[2],\n",
    "                             dot_sim_learning[1],\n",
    "                             dot_sim_mapping[3],\n",
    "                             dot_sim_learning[2],\n",
    "                             dot_sim_mapping[4]), axis=0)\n",
    "\n",
    "n_blocks = 8\n",
    "mapping_block = [0, 1, 3, 5, 7]\n",
    "learning_block = [2, 4, 6]\n",
    "\n",
    "sns.set_theme(context='notebook', style='ticks', palette='deep')   \n",
    "plt.figure()\n",
    "for i in range(n_blocks):\n",
    "    if i in learning_block:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'salmon'\n",
    "    plt.scatter(range(n_trials*i, n_trials*(i+1)),\n",
    "                cosine_sim[i*n_trials:(i+1)*n_trials],\n",
    "                # dot_sim[i*n_trials:(i+1)*n_trials],\n",
    "                color=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
